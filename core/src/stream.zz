using err;
using channel;
using log;
using slice;
using byteorder;
using io;
using time;
using buffer;
using madpack;
using mem;
using pool;
using pq;

export closure open_fn       (Stream mut* self, err::Err mut *e, slice::Slice headers);
export closure close_fn      (Stream mut* self, err::Err mut *e);
export closure stream_fn     (Stream mut* self, err::Err mut *e, slice::Slice b) -> bool;
export closure fragmented_fn (Stream mut* self, err::Err mut *e, u32 fragments) -> bool;
export closure poll_fn       (Stream mut* self, err::Err mut *e, io::Async mut *async);

export struct Config {
    char *          path;
    open_fn         open;
    close_fn        close;
    stream_fn       stream;
    fragmented_fn   fragmented;
    poll_fn         poll;
    slice::Slice    index;
}

export struct Stream {
    Config *config;
    channel::Channel mut* chan;
    u32 id;
    u64 order_incomming;
    u64 order_outgoing;

    // incomming out of order packet currently buffered
    slice::Slice incomming_reorder;
    u64 incomming_reorder_at;

    // don't catch errors. let them bubble to the top
    bool errors_are_fatal;
    bool closing;
    u64  closed_linger_until;

    u64 broker_route;

    //user
    usize       state;
    u64         user1;
    void mut*   user2;

    bool        memory_pressure;
    io::Io      memory_pressure_timer_io;
}

export fn cleanup(Stream mut * self)
{
    if self->incomming_reorder.mem != 0 {
        static_attest(safe(self->chan));
        static_attest(safe(self->chan->endpoint));
        static_attest(pool::member(self->incomming_reorder.mem, &self->chan->endpoint->framebuffer));
        self->chan->endpoint->framebuffer.free(self->incomming_reorder.mem);
        self->incomming_reorder.mem = 0;
        self->incomming_reorder.size = 0;
    }
}

export fn index(Stream *self) -> slice::slice::Slice
    model slice::slice::integrity(&return)
{
    err::assert_safe(self->config);
    static_attest(slice::slice::integrity(&self->config->index));
    return self->config->index;
}

/// allocate a frame for sending later
export fn stream(Stream mut * self, err::Err mut*e, usize reserved_size) -> slice::MutSlice
    where err::checked(*e)
    where reserved_size < 100000
    model slice::mut_slice::integrity(&return)
{
    log::debug("req stream frame with size %d", reserved_size);
    static_attest(safe(self->chan));
    let mut frame = self->chan->q.alloc(e, channel::FrameType::Stream, reserved_size + 14);
    if err::check(e) {
        if e->error == pq::MaxQ {
            self->memory_pressure = true;
        }
        unsafe {
            slice::MutSlice bad = {0};
            return bad;
        }
    }

    err::assert(
        frame.push32(byteorder::to_be32(self->id)) &&
        frame.push64(byteorder::to_be64((self->order_outgoing)++)) &&
        frame.push16(0)
    );
    return frame;
}

/// indicate the next N frames are actually one frame
export fn fragmented(Stream mut * self, err::Err mut*e, u32 fragments)
    where err::checked(*e)
{
    log::debug("req frag frame with count %d", fragments);
    static_attest(safe(self->chan));
    let mut frame = self->chan->q.alloc(e, channel::FrameType::Fragmented, 16);
    if err::check(e) {
        if e->error == pq::MaxQ {
            self->memory_pressure = true;
        }
        return;
    }

    err::assert(
        frame.push32(byteorder::to_be32(self->id)) &&
        frame.push64(byteorder::to_be64((self->order_outgoing)++)) &&
        frame.push32(byteorder::to_be32(fragments))
    );
}


/// undo last allocation without sending.
/// CAN ONLY UNDO THE LAST ALLOC
export fn cancel(Stream mut * self)
{
    static_attest(safe(self->chan));
    self->chan->q.cancel();
    (self->order_outgoing)--;
}

export fn close(Stream mut* self)
{
    if self->closing {
        return;
    }
    new+100 e = err::make();
    static_attest(safe(self->chan));
    self->chan->send_close_frame(
        &e,
        self->id,
        self->order_outgoing
    );
    if err::check(&e) {
        e.elog();
        e.make();
    }

    self->closing       = true;
    self->closed_linger_until = 0;
    if self->memory_pressure_timer_io.valid() {
        self->memory_pressure_timer_io.close();
    }
}

/// handle incomming stream frame
/// returns false if frame can currently not be accepted and needs to be resent
pub fn incomming_stream(Stream mut*self, err::Err mut *e, u64 order, slice::Slice b) -> bool
    where err::checked(*e)
    where slice::slice::integrity(&b)
{

    // try to empty the reorder queue first
    if self->incomming_reorder_at == self->order_incomming + 1 && self->incomming_reorder.mem != 0{

        u64 tmp = self->incomming_reorder_at;
        self->incomming_reorder_at = 0;

        static_attest(slice::slice::integrity(&self->incomming_reorder));
        bool r = incomming_stream(self, e, tmp, self->incomming_reorder);
        if r {
            self->incomming_reorder.mem  = 0;
            self->incomming_reorder_at   = 0;
        } else {
            self->incomming_reorder_at = tmp;
        }
        if err::check(e) {
            return r;
        }
    }



    if self->closing {
        return true;
    }

    if order <= self->order_incomming {
        log::debug("ignoring dup %d", order);
        // already seen, dont resend
        return true;
    }
    if order != self->order_incomming + 1 {

        // if this is +2 , buffer it until we received + 1
        if order == self->order_incomming + 2 && self->incomming_reorder.mem == 0 {
            static_attest(safe(self->chan));
            static_attest(safe(self->chan->endpoint));
            u8 mut * bcopy = self->chan->endpoint->framebuffer.malloc(b.size);
            if bcopy == 0 {
                return false;
            }
            unsafe{mem::copy(b.mem, bcopy, b.size);};
            self->incomming_reorder.mem  = bcopy;
            self->incomming_reorder.size = b.size;

            self->incomming_reorder_at = order;

            return true;
        }


        // TODO buffer
        // cannot accept, resend later
        return false;
    }

    if self->config == 0 {
        self->order_incomming += 1;
        return true;
    }
    static_attest(safe(self->config));

    if self->config->stream.fn == 0 {
        self->order_incomming += 1;
        return true;
    }
    static_attest(safe(self->config->stream));

    bool accepted = self->config->stream(self, e, b);
    if accepted {
        self->order_incomming += 1;
    }
    return accepted;
}

pub fn incomming_fragmented(Stream mut*self, err::Err mut *e, u64 order, u32 fragments) -> bool
    where err::checked(*e)
{
    if self->closing {
        return true;
    }

    if order <= self->order_incomming {
        log::debug("ignoring dup %d", order);
        // already seen, dont resend
        return true;
    }
    if order != self->order_incomming + 1 {
        // TODO buffer
        // cannot accept, resend later
        return false;
    }

    if self->config == 0 {
        self->order_incomming += 1;
        return true;
    }
    static_attest(safe(self->config));

    if self->config->fragmented.fn == 0 {
        self->close();
        e->fail(err::NotImplemented, "stream does not support fragments");
        return false;
    }
    static_attest(safe(self->config->fragmented));

    bool accepted = self->config->fragmented(self, e, fragments);
    if accepted {
        self->order_incomming += 1;
    }
    return accepted;
}


pub fn incomming_close(Stream mut*self, err::Err mut *e, u64 order) -> bool
    where err::checked(*e)
{
    if self->closing {
        return true;
    }

    if order <= self->order_incomming {
        // already seen, dont resend
        return true;
    }
    if order != self->order_incomming + 1 {
        // TODO buffer
        // cannot accept, resend later
        return false;
    }
    self->order_incomming += 1;

    self->close();

    return true;
}

pub fn do_poll(Stream mut*self, err::Err mut *e, io::Async mut*async)
    where err::checked(*e)
{
    if self->closing  || self->config == 0{
        return;
    }

    if self->memory_pressure{
        self->memory_pressure_timer_io.close();
        self->memory_pressure_timer_io = async->timeout(e, time::from_millis(100));
        if err::check(e) { return; }
        self->memory_pressure = false;
    }
    if self->memory_pressure_timer_io.valid() {
        new+20 s = buffer::make();
        if self->memory_pressure_timer_io.read(e, &s) == io::Result::Later {
            return;
        } else {
            self->memory_pressure_timer_io.close();
        }
    }

    static_attest(safe(self->config));
    if self->config->poll.fn != 0 {
        static_attest(safe(self->config->poll));
        self->config->poll(self, e, async);
        if err::check(e) {
            if self->errors_are_fatal {
                return;
            } else {
                if e->error != pq::MaxQ {
                    e->elog();
                }
                e->ignore();
            }
        }
    }
}
