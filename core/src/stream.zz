using err;
using channel;
using log;
using slice;
using byteorder;
using io;
using time;
using buffer;

export closure open_fn       (Stream mut* self, err::Err+et mut *e, slice::Slice headers);
export closure close_fn      (Stream mut* self, err::Err+et mut *e);
export closure stream_fn     (Stream mut* self, err::Err+et mut *e, slice::Slice b) -> bool;
export closure fragmented_fn (Stream mut* self, err::Err+et mut *e, u32 fragments) -> bool;
export closure poll_fn       (Stream mut* self, err::Err+et mut *e, io::Async mut *async);

export struct Config {
    char *          path;
    open_fn         open;
    close_fn        close;
    stream_fn       stream;
    fragmented_fn   fragmented;
    poll_fn         poll;
}

export struct Stream {
    Config *config;
    channel::Channel mut* chan;
    u32 id;
    u64 order_incomming;
    u64 order_outgoing;

    // don't catch errors. let them bubble to the top
    bool errors_are_fatal;
    bool closing;
    u64  closed_linger_until;

    u64 broker_route;

    //user
    usize       state;
    u64         user1;
    void mut*   user2;

    bool        memory_pressure;
    io::Io      memory_pressure_timer_io;
}


/// allocate a frame for sending later
export fn stream(Stream mut * self, err::Err+et mut*e, usize reserved_size) -> slice::MutSlice
    where err::checked(*e)
    where reserved_size < 100000
    model slice::mut_slice::integrity(&return)
{
    static_attest(safe(self->chan));
    let mut frame = self->chan->q.alloc(e, channel::FrameType::Stream, reserved_size + 14);
    if err::check(e) {
        if e->error == err::OutOfTail {
            self->memory_pressure = true;
        }
        unsafe {
            slice::MutSlice bad = {0};
            return bad;
        }
    }

    err::assert(frame.push32(byteorder::to_be32(self->id)));
    err::assert(frame.push64(byteorder::to_be64((self->order_outgoing)++)));
    err::assert(frame.push16(0));
    return frame;
}


/// undo last allocation without sending.
/// CAN ONLY UNDO THE LAST ALLOC
export fn cancel(Stream mut * self)
{
    static_attest(safe(self->chan));
    self->chan->q.cancel();
    (self->order_outgoing)--;
}

export fn close(Stream mut* self)
{
    if self->closing {
        return;
    }
    new+100 e = err::make();
    static_attest(safe(self->chan));
    self->chan->send_close_frame(
        &e,
        self->id,
        self->order_outgoing
    );
    if err::check(&e) {
        e.elog();
        e.make();
    }

    self->closing       = true;
    self->closed_linger_until = 0;
    if self->memory_pressure_timer_io.valid() {
        self->memory_pressure_timer_io.close();
    }
}

pub fn incomming_stream(Stream mut*self, err::Err+et mut *e, u64 order, slice::Slice b) -> bool
    where err::checked(*e)
    where slice::slice::integrity(&b)
{
    if self->closing {
        return true;
    }

    if order <= self->order_incomming {
        log::debug("ignoring dup %d", order);
        // already seen, dont resend
        return true;
    }
    if order != self->order_incomming + 1 {
        // TODO buffer
        // cannot accept, resend later
        return false;
    }

    if self->config == 0 {
        self->order_incomming += 1;
        return true;
    }
    static_attest(safe(self->config));

    if self->config->stream.fn == 0 {
        self->order_incomming += 1;
        return true;
    }
    static_attest(safe(self->config->stream));

    bool accepted = self->config->stream(self, e, b);
    if accepted {
        self->order_incomming += 1;
    }
    return accepted;
}

pub fn incomming_fragmented(Stream mut*self, err::Err+et mut *e, u64 order, u32 fragments) -> bool
    where err::checked(*e)
{
    if self->closing {
        return true;
    }

    if order <= self->order_incomming {
        log::debug("ignoring dup %d", order);
        // already seen, dont resend
        return true;
    }
    if order != self->order_incomming + 1 {
        // TODO buffer
        // cannot accept, resend later
        return false;
    }

    if self->config == 0 {
        self->order_incomming += 1;
        return true;
    }
    static_attest(safe(self->config));

    if self->config->fragmented.fn == 0 {
        self->order_incomming += 1;
        return true;
    }
    static_attest(safe(self->config->fragmented));

    bool accepted = self->config->fragmented(self, e, fragments);
    if accepted {
        self->order_incomming += 1;
    }
    return accepted;
}


pub fn incomming_close(Stream mut*self, err::Err+et mut *e, u64 order) -> bool
    where err::checked(*e)
{
    if self->closing {
        return true;
    }

    if order <= self->order_incomming {
        // already seen, dont resend
        return true;
    }
    if order != self->order_incomming + 1 {
        // TODO buffer
        // cannot accept, resend later
        return false;
    }
    self->order_incomming += 1;

    self->close();

    return true;
}

pub fn do_poll(Stream mut*self, err::Err+et mut *e, io::Async mut*async)
    where err::checked(*e)
{
    if self->closing  || self->config == 0{
        return;
    }

    if self->memory_pressure{
        self->memory_pressure_timer_io.close();
        self->memory_pressure_timer_io = async->timeout(e, time::from_millis(100));
        if err::check(e) { return; }
        self->memory_pressure = false;
    }
    if self->memory_pressure_timer_io.valid() {
        new+20 s = buffer::make();
        if self->memory_pressure_timer_io.read(e, &s) == io::Result::Later {
            return;
        } else {
            self->memory_pressure_timer_io.close();
        }
    }

    static_attest(safe(self->config));
    if self->config->poll.fn != 0 {
        static_attest(safe(self->config->poll));
        self->config->poll(self, e, async);
        if err::check(e) {
            if self->errors_are_fatal {
                return;
            } else {
                e->elog();
                e->make();
            }
        }
    }
}
